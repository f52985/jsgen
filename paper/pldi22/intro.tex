\section{Introduction}\label{sec:intro}

JavaScript~\cite{js-hopl} has quickly evolved with an annual release cadence and
open development process since 2015. ECMAScript is the standard specification of
JavaScript maintained by the Ecma Technical Committee 39 (TC39). In June 2015,
the committee introduced its new version called ECMAScript 6 (ES6,
2015)~\cite{es6}. It was the most extensive single update with new language
features, such as classes, arrow functions, promises, iterators, generators, and
proxies. Moreover,  to quickly adapt users' demands to the specification, TC39
members decided to release ECMAScript yearly and maintain it as an open-source
project in a GitHub repository\footnote{https://github.com/tc39/ecma262}.
Therefore, already six more versions of ECMAScript from ES7 to ES12 have been
released after ES6, and contributors have pushed \inred{2,150} commits to its
official GitHub repository.

However, most JavaScript engines did not support or partially supported newly
introduced features in the beginning. For example, even though ES6 was released
in June 2015, Google introduced Chrome 51, the first web browser that fully
supported ES6 features, in May 2016. Subsequently, Safari 10 (Apple, Sep. 2016)
and Firefox 54 (Mozilla, Sep. 2017) fully supported ES6 features. Thus,
client-side developers cannot use most ES6 features initially for the web
application development, although they wanted to utilize them for more concise
and readable JavaScript code.

Several \textit{transpilers} showed up to fulfill developers' desires to use new
language features earlier. They focused on how to \textit{desugar} newly
introduced language features in ES6+ to ECMAScript 5 (ES5, 2009). In 2011,
Google started to develop a transpiler called
Traceur\footnote{https://github.com/google/traceur-compiler} for experimenting
with early ES6 features, and it provided a high-fidelity implementation of ES6
semantics. However, its severe speed degradation was unattractive to developers.
On the other hand, a 17-year-old developer introduced another transpiler
Babel\footnote{https://babeljs.io/} (originally named 6to5) in 2015 to minimize
runtime overhead of desugared JavaScropt programs by sacrificing semantics
preservation. Nowadays, Babel has become an essential library to build
JavaScript projects with 32.5M weekly downloads and 16.7K npm packages dependent
on it\footnote{https://www.npmjs.com/package/@babel/core}. Even though most
engines now support language features even in the latest ECMAScript, developers
still heavily utilize Babel to execute their JavaScript programs in legacy
engines: older versions of engines, deprecated engines (e.g., Internet
Explorer), or regional web browsers (e.g., UC and QQ browsers in China) not
fully supporting language features in ES6+. However, unfortunately, there is no
empirical study on such desugaring process of transpilers, especially for Babel.

This paper takes the first systematically studying the desugaring process of
Babel, the most dominating JavaScript transpiler. We measure the quality of
desugaring process from ES6 or later versions to ES5 based on its
\textit{semantics preservation}, \textit{speed degradation}, and \textit{code
bloat}. First, the essential property of desugaring process is to preserve
semantics. However, no one confidently claims which features are not correctly
desugared by Babel. Second, even though Babel sacrifices semantics preservation
to overcome the runtime overhead of desugared JavaScript programs, speed
degradation still exists in the desugaring process. Besides, it is unclear when
the speed degradation becomes serious. Third, sizes of JavaScript programs are
also important because small-sized JavaScript programs speed up file transfer
and decrease costs for network bandwidth. If the desugaring process explodes the
size of JavaScript code, it reduces the usability of JavaScript applications
because of slow code transmission.

We develop $\tool$, a \textbf{J}ava\textbf{S}cript Program \textbf{Gen}erator
using genetic algorithms, to automatically measure the quality and find the
worst cases in each metric. To evenly treat each language feature under the same
granularity, we first introduce a solid definition of JavaScript language
features and an automated approach to extract them from JavaScript programs
using their statistical information. We utilize a variant of Term
Frequency-Inverse Document Frequency (TF-IDF)~\cite{tf, idf} widely used in
natural language processing. Then, we automatically generate JavaScript
programs using our tool and measure the semantics preservation, speed
degradation, and code bloats of Babel's desugaring process with them. Moreover,
$\tool$ can find the worst cases in each metric by generating JavaScript
programs using genetic algorithms~\cite{ga1, ga2, ga3, ga4} with different
fitness functions for distinct metrics.

Contributions of this work are as follows:
\begin{itemize}

  \item We conduct the first empirical study on the desugaring process of Babel,
    the most dominating JavaScript transpiler, with three metrics:
    \textit{semantics preservation}, \textit{speed degradation}, and
    \textit{code bloat}.

  \item We develop $\tool$, a \textbf{J}ava\textbf{S}cript Program
    \textbf{Gen}erator using genetic algorithms, to automatically measure the
    quality and find the worst cases in each metric.

  \item We experimentally show that Babel incorrectly desugars \inred{XX}
    language features, degrades the execution speed by \inred{XX.X}\% on
    average, and bloats code sizes by \inred{XX.X}\% on average. In the worst
    case, the speed was \inred{XX.X} times slower, and the size was \inred{XX.X}
    times increased.
\end{itemize}
